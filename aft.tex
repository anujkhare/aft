\documentclass[12pt,a4paper]{report}

\usepackage{amsthm,amssymb,mathrsfs,setspace}%amsmath, latexsym,footmisc

% \usepackage{pstcol}
% \usepackage{play}
\usepackage[document]{ragged2e}
\usepackage{epsfig}
%\usepackage[grey,times]{quotchap}
\usepackage[nottoc]{tocbibind}
\usepackage{amsmath}
%% CREATE BOOKMARKS IN THE PDF!
\usepackage[bookmarks]{hyperref}
%\usepackage[hidelinks]{hyperref}
\usepackage{algorithm2e}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
%

\input xy
\xyoption{all}

\begin{document}
\section{Regularized AFT models}

We use the log-logistic model. The CDF is given as follows:
\begin{equation} \label{cdf}
F(t;\alpha, \beta) = \frac{1}{1+(t/\alpha)^{-\beta}}
\end{equation}

The probability density function is given as:
\begin{equation} \label{pdf}
d(t;\alpha, \beta) = \frac{(\beta / \alpha)(t/{\alpha})^{\beta -1 }}{(1+(t/\alpha)^{-\beta})^2}
\end{equation}

Survival and hazard functions can be calulated from the above:
\begin{equation} \label{survival}
s(t;\alpha, \beta) = \frac{1}{1+(t/\alpha)^{\beta}}
\end{equation}

\begin{equation} \label{hazard}
h(t;\alpha, \beta) = \frac{ (\beta/\alpha) (t/\alpha) ^ {\beta-1}}
						  {1+(t/\alpha)^{\beta}}
\end{equation}


For survival analysis, given time intervals $\{\underline t_i, \overline t_i\}$ and covariates $x_i$ for $i=1:n$,
the likelihood is given as

\begin{equation}
L(w) = \prod_{i=1}^n \zeta(w, \underline t_i, \overline t_i)
\end{equation}

where,
\begin{equation} \label{zeta}
\zeta(w, \underline t_i, \overline t_i) = \begin{cases}
													   d(t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i = \overline t_i<\infty \\
													   F(\overline t_i, \alpha_i, \beta) & \mbox{if: } -\infty = \underline t_i \mbox{ , } \overline t_i<\infty \\
													   s(\underline t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i \mbox{ , }\overline t_i=\infty \\
													   F(\overline t_i, \alpha_i, \beta) - F(\underline t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i \ne \overline t_i<\infty \\
		\end{cases}
\end{equation}

where, $\alpha_i = e^{w^Tx_i}$. %, and the link function is the identity function.

In the observations with no censoring, the pdf is used, and in censored observations,
the cdf is used.

%\begin{equation} \label{deltas}
%(\delta_l, \delta_r) = 
%		\begin{cases} (0, 0) & \mbox{: Interval censoring} \\
%					  (0, 1) & \mbox{: Left censoring} \\
%					  (1, 0) & \mbox{: Right censoring} \\
%					  (1, 1) & \mbox{: No censoring} \\
%		\end{cases}
%\end{equation}


%\begin{equation} \label{lik0}
%L(w) = \prod_{i=1}^n [d(t_{r_i})]^{\delta_{l_i} \delta_{r_i}}
%					 [F(t_{l_i})]^{(1-\delta_{l_i}) \delta_{r_i}}
%					 [s(t_{r_i})]^{\delta_{l_i} (1-\delta_{r_i})}
%					 [s(t_{l_i}) - s(t_{r_i})]^{(1-\delta_{l_i}) (1-\delta_{r_i})}
%\end{equation}


%Substituting from the previous equations and simplyfing, we obtain the following log-likelihood:
Hence the log likelihood is given as

\begin{equation} \label{lik0}
l(w) = \sum_{i=1}^n \log \zeta(w, \underline t_i, \overline t_i) = \sum_{i=1}^n \ell_i %(w, \underline t_i, \overline t_i)
%\begin{split}
%l(w) = \sum_{i=1}^n \{{\delta_{l_i} \delta_{r_i}} \log(\beta / {t_{l_i}})
%					 - {\delta_{r_i}} \log (1+ (t_{r_i}/\alpha_i)^{\beta}) \\
%					 - {\delta_{l_i}} \log ((\alpha_i^{\beta} + t_{r_i}^{\beta}) / t_{l_i}^{\beta})
%					 + \log (\alpha_i^{\beta} + t_{r_i}^{\beta}) - \log (\alpha_i^{\beta} + t_{l_i}^{\beta})
%					 \}
%\end{split}
\end{equation}

From \ref{zeta} and \ref{lik0}, the partial derivative of the log-likelihood wrt to $w_j$ is given as:
%\begin{equation} \label{lld}
%\frac{\partial{l(w)}}{\partial{\alpha}} = \sum_{i=1}^{n} \frac{\partial{\ell_i}}{\partial{\alpha_i}}
%%\frac{\partial{l(w)_i}}{\partial{\alpha_i}} = \delta_{r_i} \frac{\beta / \alpha_i} {1 + (t_{r_i} / \alpha_i) ^ \beta}
%%											  - \delta_{l_i} \frac{\beta / \alpha_i} {1 + (t_{r_i} / \alpha_i) ^ \beta}
%%											  + \frac{\beta / \alpha_i} {1 + (t_{r_i} / \alpha_i) ^ \beta}
%%											  - \frac{\beta / \alpha_i} {1 + (t_{l_i} / \alpha_i) ^ \beta}
%\end{equation}

%Hence, partial derivative wrt $w$ is:
\begin{equation} \label{lld}
\frac{\partial{l(w)}}{\partial{w_j}} = \sum_{i=1}^{n} \frac{\partial{\ell_i}}{\partial{w_j}}
									 = \sum_{i=1}^{n} \frac{\partial{\ell_i}}{\partial{\alpha_i}} x_{ij} e^{w^Tx_i}
\end{equation}

%where,
%\begin{equation} \label{dl}
%\frac{\partial l_i(w, \underline t_i, \overline t_i)}{\partial \alpha_i} = \begin{cases}
%													   d(t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i = \overline t_i<\infty \\
%													   F(\overline t_i, \alpha_i, \beta) & \mbox{if: } -\infty = \underline t_i \mbox{ , } \overline t_i<\infty \\
%													   s(\underline t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i \mbox{ , }\overline t_i=\infty \\
%													   F(\overline t_i, \alpha_i, \beta) - F(\underline t_i, \alpha_i, \beta) & \mbox{if: } -\infty < \underline t_i \ne \overline t_i<\infty \\
%		\end{cases}
%\end{equation}

Adding the elastic net (L1 + L2) penalty, we get the following optimization objective:
\begin{equation} \label{objective}
f(w) = -l(w) + (\lambda_1 \|w\|_1 + 1/2 \lambda_2 \|w\|_2^2)
\end{equation}

The subderivative of the optimization objective is given as:
\begin{equation} \label{subderivative}
\frac{\partial{f}}{\partial{w_j}} = \begin{cases}
										\{-\frac{\partial{l(w)}}{\partial{w_j}} + \lambda_2 w_j -\lambda_1\} & \mbox{if }  w_j<0 \\
										[-\frac{\partial{l(w)}}{\partial{w_j}} -\lambda_1, -\frac{\partial{l(w)}}{\partial{w_j}} +\lambda_1] & \mbox{if }  w_j=0 \\
										\{-\frac{\partial{l(w)}}{\partial{w_j}} + \lambda_2 w_j +\lambda_1\} & \mbox{if } w_j>0 \\
									\end{cases}
\end{equation}

Using the subderivative, upto three cases of solutions for $w_j$ may be obtained. The coordinate descent algorithm works by
cycling through each $w_j$ in turn, keeping the others constant, and using the above estimate to calculate the optimal value
$w^*_j$. This is repeated until convergence.

\vspace{8mm}
\textbf{Sources:}
\begin{itemize}
\item Machine Learning: A Probabilistic Perspective by Kevin Murphy
\item AFT: TD Hocking
\end{itemize}

\end{document}
